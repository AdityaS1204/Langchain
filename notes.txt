LANGCHAIN:
open source framework for developing applictions powered by large language models(llms).

why we need langchain ?

alternatives of langchain ?

use of langchain ?

LANGCHAIN COMPONENTS:

1) Models
2) Prompts
3) Chains
4) Indexes
5) memory
6) Agents


1) Models:
models act as an interface making the interaction with llm models easy and makes the process of changing models easy with its inbuild functions.
There are two types of models:
1) language/chat models.
2) Embedding models.

Language/chat Models:
used for interacting with llms.
takes text and returns text.

ref:https://python.langchain.com/docs/integrations/chat/

Embedding models:
takes text and returns vector.
used for converting text into embeddings or vectors and semantic search.

ref: https://python.langchain.com/docs/integrations/text_embedding/


2) Prompt:
Types of prompt that can be created using langchain:

1. Dynamic and reusable prompts:
ex: 'summarize { topic } in {emotion } tone'.

2. Role Based Prompts:
ex: system : you are an experienced xyz
user: tell me about xyz in xyz

3. Few Shot Prompting:
 we give some examples of input and output of a prompt and ask the model to give output for next prompts in that similar way.

ex: 
input: "can you explain how to upgrade my plan ?", output: "General Inquiry"

steps:
examples
step 2 create an example template
step 3: Build the few shot prompt template.

3) Chains

chain are used to create pipelines for a bigger tasks,
for example to create a english to hindi summarization app we need to give text input to llm it trnaslate to hindi which is given as input to another llm which summarizes the text.
If using Chains the chain handles all the linking as it will make the output of one block as the input of another making the pipeline creation process less hastle.

We can make complex pipeline using chains like:
- parellel chain
- Conditional chains


4) Indexes:
Indexes connect your applications to extenals knowledges - such as pdf,wbsites or dbs.

indexes are made up of 
- Doc loader
- text splitter
- vector store 	 
- retrivers

5) Memory:
LLM API calls are stateless means every api call made to the model is independent of itself means it does not have any knowledge of past conversions.
here langchain memory component helps.

• ConversationBufferMemory: Stores a transcript of recent messages. Great for
short chats but can grow large quickly.
• ConversationBufferWindowMemory: Only keeps the last N interactions to avoid
excessive token usage.
• Summarizer-Based Memory: Periodically summarizes older chat segments to keep
a condensed memory footprint.
• Custom Memory: For advanced use cases, you can store specialized state (e-&.,
the user's preferences or key facts about them) in a custom memory class.


6) Agents:
Agents have reasoning capabilities and access to tools.

agents break a task in several task using various reasoning methods like:
chain of thoughts:
break a task into sub tasks.
for ex:
can you multiply delhi temperature by 3
using chai of thought:
--> [delhi temp]
--> [multiply by 3]


-----------------------------------------------------------------------
MODELS:
using langchain we can interact with two types of models:
-- language models
• LLMs
• Chat Models

-- embedding models

-> Language Models:
LLMs:
general purpose models used for raw text generations.they take string or plain text as input and give string orr plain text as output.

Chat Models:
Language models that are specialized for conversationals tasks. they take sequence of message as input and return chat messages as output. these are used more in comparison to llms. they are traditional newer models.


temperature: is a parameter that define the randomness of a language models output. It affects how creative or deterministic the response are. It ranges from 0 to 2.
lower values (0.0 - 0.3) : more deterministic and predictable o/p.
higner values (0.7 - 1.5) : more random creative and diverse.


Open Source Models:
Open source models are freely available models that can be modified , fine tuned, deployed without any restrictions by central providers.

Ways to use opensource models:
-- download locally
-- use HF inference API

disadvantages:
high hardware requirements: requires gpus to run models.
setup complexity: to run models it requires some dependencies like pytorch, cuda, transformers, etc.
lack of RLHF:opensource models lack fine tuning on human feedback making them weak in instruction following.
Limited multimodel abilities: Open model dont support images, audio or video input like close models.

Vector Embedding:
Vector embeddings are multi-dimensional numerical representations of text (such as words, sentences, or documents) that capture their semantic meaning in a way that can be processed by machine learning models.

semantic search:


----------------------------------------------------------------------------
PROMPTS:
Prompts are inputs instructions or queries given to models to guide its output.

Static prompts:
static prompts are the simple fixed prompts
ex: "summarize attention is all you need paper"

Dynamic Prompts:
dynamic prompts have the key vlaues entered by user, in dynamic prompts variables can be used to make the prompt dynamic.
Ex:
"please summarize the research paper titled "(paper_input)" with the following specifications:
explaination style:{style_input}
explaination length:(lenght_input)
1. Mathematical Details:
- include relevent mathematical equations if present in the paper.
- Explain mathematical concepts using simple , intuitive code snippets where aplicable.
2. analogies:
- use relatable analogies to simplify complex ideas.
If certain information is not avaliable in the paper respond with: "insuffcient Information available" instead of guessing.
insure that the summary provided is correct, acurate , clear and aligned with the provided style and length."

Prompt Template:
In langchain we can create dynamic prompts using PromptTemplate function.
- used in single turn messages.
- PromptTemplate function makes the prompt reusable.
- we can reuse the prompt template by creating a Json file and can load it where ever needed.(using load_prompt form langchain-core.prompts)
- have functions to validate prompt template using validate_template attribute.
These are the main features that makes PromptTemplate a better alternative to f string present in python.

Chat prompt Template:
For creating dynamic prompt in a multiturn conversation/messages we have chat_promopt_template in langchain.
- It simplify the process for creating a dynamic prompt for system, human or AI.
- all the featues like prompt template.


Messages:
Messages help structure the chat history by clearly labeling input from the user and responses from the AI, as well as any system instructions.
The main types are:
HumanMessage - user input
AIMessage - AI response
SystemMessage - prompts or instructions to guide AI behavior

Message Placeholder:
A MessagesPlaceholder is used to insert chat history into a prompt at runtime. It allows you to include past messages (like user and AI exchanges) in ongoing or resumed conversations. It's useful when you want the model to consider previous context, even if the conversation was paused or cached.



-------------------------------------------------------------------------------------------------
OUTPUTS:

Structured Output:
In Langchain structured outputs refers to the practice of having an llm model return response in well defined structure or data format (for example JSON) rather than free format text. This makes the model output easeir to parse and use programmatically.

Ways to get structured outputs:
-- llms that can generate structured outputs: we can use [with_structured_output function] in langchain.
-- llms that cannot generate structured outputs: we use output parsers to parse the output.

with_structured_output fuction:
we call with_structured_output function with the data format before invoking the model.
when we use with_structured_output function we can get output in two way using the method parameter one is json mode and another is function calling in json mode the output is in json format and function calling is used when we want to call a function as an output used for agent to use tools.

WE can define data format using three ways:

1) TypedDict:
- In typed dictonaries we define a class with datatypes defined for each key and then we create dictonaries using the class it tells what keys are required and what should be the data type of the value.
- it does not validate the data it will just show the required data type for the value but does not force the data to be of the the same data type means i can assign a str value to a number type value it will not through error.
- we can use Annotated to describe the use of the key for ex:
class review(TypedDict):
    summary:Annotated[str,"description for the llm to understand what to return for this key"]
    
2) Pydantic: It is an data validation and data parseing library for python. It ensures that the data you work with is correct ,structured and type safe.
-- Can pass default values
-- optional fields: we can give an optional type to an integer like this => age: optional[int] = None , or optional[int] = str 
-- coercion is implicit type conversion in python but here pydantic add one more feature over it by converting the input data type to the expected types defined in pydantic model.

3) JSON_schema: 
- create a json schema for the output and pass it to the with_structured_output function.
- mostly used when there are more than one language being used in the software.

When to use What ?
use typeddict if:
- you only need type hints.
- you trust the llm to return correct data.
- you dont need data validation.

Use Pydantic if:
- you want data validation (eg: stentiment must be '+ve','-ve' or 'neutral')
- you want default values if the llm misses any fields.
- you want automatic type conversation.

Use json schema if:
- you dont want to import extra python libraries.
- you need validation but dont want python objects.
- you want to define structure in a standard json format.
- when you are working with multiple language in backend.

OUTPUT PARSERs:

Output parsers are used for those llms which dont have abilities to give structured output.
so langchain provides this output parser feature to add that ability to them.

Types of output parser:

1) str output parser:
- generally used with chains.
2) Json output parser:
- cannot decide/enforce the json schema, here the llm decides the schema by default.
3) structured output parser:
- structured outpt parser is an output parser in lnagchain that helps extract structured JSON data from llm response based on predfined fields schema.
- no data validation.
4) pydantic output parser:
- pydantic output parser uses pyadntic models to enforce schema validation when parsing llm output.
- strict schema enforcement
- easy validation.
- type safety.
- seamless integrations.

CHAINS:

Chains in LangChain are used to combine different components like prompts, models, parsers, etc., in a structured way so you can automate complex tasks step by step. Think of it as chaining different steps of a pipeline together.

types of chains:

1) sequential chain: 
every part in the chain is executed in sequence.Executes each step one after another.

ex: chain = prompt1 | model | parser | prompt2 | model | parser

2) parellel chain: 
Runs multiple chains at the same time (in parallel) for different purposes, then combines the results.

ex: parellel_chain = RunnableParallel({
    'notes': prompt1 | model | parser,
    'quiz': prompt2 | model | parser
})
merge_chain = prompt3 | model | parser
chain = parellel_chain | merge_chain

3) conditional chain:
Uses logic (if/else conditions) to decide what to do next based on the input or previous result.

ex: classifier_chain = prompt1 | model | parser2

branch_chain = RunnableBranch(
    (lambda x: x.sentiment == 'positive', prompt2 | model | parser),
    (lambda x: x.sentiment == 'negative', prompt3 | model | parser),
    RunnableLambda(lambda x: "could not find sentiment")
)

chain = classifier_chain | branch_chain

 RUNNABLES

Types of runnables:
1) Task specific runnables
2) Runnable Primitive

Runnable Primitive:
1.Runnable Sequence:
◦Purpose: Helps to sequentially connect two or more runnables.

◦How it works: The output of the first runnable automatically becomes the input for the next runnable in the sequence. There is no restriction on how many runnables can be connected.
◦This is conceptually similar to the RunnableConnector built in the previous video.

◦Example Use Case: Creating a chain where a PromptTemplate's output goes to an LLM, and the LLM's output goes to a StringOutputParser to generate a joke. It can also extend to more complex sequential flows, like generating a joke and then explaining it by passing the joke to another prompt, then an LLM, and finally a parser.

2.Runnable Parallel:
◦ Purpose: Helps to create parallel chains, allowing multiple runnables to execute simultaneously.

◦ How it works:
▪ Each runnable in the parallel chain receives the same input.
▪ They process the input independently and in parallel.
▪ The output is a dictionary of outputs, where each key corresponds to the output of a specific parallel branch.

◦ Example Use Case: Taking a topic (e.g., "AI") and simultaneously sending it to two different LLMs (or the same model with different prompts). One LLM generates a tweet, and the other generates a LinkedIn post. Both outputs are then received as a dictionary.

3.Runnable Passthrough:
◦ Purpose: A special runnable primitive that returns its input as its output, without any modification or processing.

◦ How it works: If you send "2" as input, it returns "2". If you send a dictionary, it returns the exact same dictionary.

◦ Usefulness: It is useful in scenarios where you need to pass an intermediate output through a chain without altering it, while other branches might be processing that output.

◦ Example Use Case: In a chain that generates a joke and its explanation, RunnablePassthrough can be used in a parallel branch to ensure the generated joke is also outputted, alongside the explanation, as part of a dictionary, because the default sequential chain would only output the final explanation.

4.Runnable Lambda:
◦ Purpose: Allows you to convert any Python function into a runnable.

◦ How it works: Once a Python function is converted into a runnable, it can be seamlessly integrated into any chain, just like other runnables. This enables the inclusion of custom logic or pre-processing steps directly within a LangChain workflow.

◦ Example Use Case: In a joke generation application, after generating the joke, a RunnableLambda can be used in a parallel branch to count the number of words in the joke (a task LLMs are not good at). The RunnableLambda encapsulates a Python function for word counting, allowing both the joke and its word count to be returned.

5. Runnable Branch:
◦ Purpose: Used for creating conditional chains, acting as an "if-else" statement within LangChain's ecosystem.

◦ How it works: It takes a series of conditions (defined by lambda functions) and corresponding runnables (chains). Based on which condition evaluates to true, the respective runnable (branch) is triggered. A default condition can be provided for cases where none of the explicit conditions are met.

◦ Example Use Case: Processing customer emails based on their content (e.g., complaint, refund, general query) and triggering different subsequent actions. Another example is generating a report and then conditionally summarising it if it exceeds a certain word count (e.g., 500 words), or simply printing it as is if it's shorter.
LangChain Expression Language (LCEL)

• Concept: LangChain creators observed that RunnableSequence is very frequently used to build sequential chains.

• Purpose: To provide a simpler, more declarative way to define sequential chains.

• Current Implementation: Instead of instantiating RunnableSequence explicitly, you can now use the pipe (|) operator to chain runnables, like R1 | R2 | R3. This syntax is known as LCEL.